# -*- coding: utf-8 -*-
"""
Code Introduction:
    This code 
Version History:
    Created: Sat Apr  6 10:06:15 2019
    Current: 

@author: Xing Guo (xingguo@umich.edu)

"""

#%% Import Moduels

## System Tools
import os
import numpy as np
## I/O Tools
import _pickle as pickle
## Data Process Tools
import pandas as pd
import datetime
## Graphs
import matplotlib.pyplot as plt
import matplotlib.backends.backend_pdf as figpdf
import matplotlib.dates as matdates
## Statistical Tools
import statsmodels.api as sm
## Database API
from fredapi import Fred
## API for WRDS
import wrds

# End of Section: Import Moduels
###############################################################################

#%% Setup Working Directory


## Windows System Path
FolderList = [xx + "EquityMarkets_MonetaryPolicy\\Data\\Micro_data\\codes" \
              for xx in ["E:\\Dropbox (Bank of Canada)\\Research Projects\\", \
                         "B:\\Dropbox (Bank of Canada)\\Research Projects\\"] ]
for Folder in FolderList:
    if os.path.exists(Folder):
        os.chdir(Folder)    


# End of Section: Setup Working Directory
###############################################################################


#%% Construct the Link between SDC Deals and Compustat ID


### Original Samples 
 
## Read-in the SDC Sample
DataFolder      =   "..\\temp\\"
SDC             =   pickle.load(open(DataFolder+'SDC_IssuanceInfo.p','rb'))

## Read-in the List of Firms in Compustat
'''
Vintage
DataFolder      =   "..\\..\\..\\..\\..\\Data\\Compustat\\Quarterly\\"
CS              =   pickle.load(open(DataFolder+"Compustat_Names.p","rb"))
'''
DataFolder      =   "..\\..\\..\\..\\..\\Data\\Compustat\\Dataset\\"
## SECURITY File
SECURITY        =   pickle.load(open(DataFolder+"comp_security.p",'rb'))
# Preliminary Cleaning
'''
'excntry' and 'exchg' records the country and market where the stock is exchanged.
'''
SECURITY[['gvkey','excntry']] = SECURITY[['gvkey','excntry']].astype(str)
SECURITY['exchg'] = pd.to_numeric(SECURITY['exchg'],downcast='signed',errors='coerce')

SECURITY = SECURITY.dropna(subset=['gvkey']).reset_index(drop=True)

CS              =   SECURITY[['gvkey','cusip']].dropna().drop_duplicates() \
                    .reset_index(drop=True)

### Search in the WRDS by Matching the CUSIP
Link_SDC_CS     =   SDC[['IssueID','CUSIP','CUSIP_8digit']].reset_index(drop=True)

TempCounter     =   0
for ii in Link_SDC_CS.index:
    TempCusip               =   Link_SDC_CS.loc[ii,'CUSIP_8digit']
    TempSearchIndex         =   CS['cusip'].str.startswith(TempCusip)
    Temp_TargetNum          =   TempSearchIndex.sum()
    Link_SDC_CS.loc[ii,'LinkNum']   =   Temp_TargetNum
    if Temp_TargetNum==0:
        Link_SDC_CS.loc[ii,'gvkey']     =   'nan'
        Link_SDC_CS.loc[ii,'CUSIP_CS']  =   'nan'
    if Temp_TargetNum==1:
        TempCounter                     =   TempCounter+1
        Link_SDC_CS.loc[ii,'gvkey']     =   CS.loc[TempSearchIndex,'gvkey'].values[0]
        Link_SDC_CS.loc[ii,'CUSIP_CS']  =   CS.loc[TempSearchIndex,'cusip'].values[0]
    if Temp_TargetNum>1:
        TempCounter                     =   TempCounter+1
        Link_SDC_CS.loc[ii,'gvkey']     =   ','.join(CS.loc[TempSearchIndex,'gvkey'] \
                                                     .unique().tolist())
        Link_SDC_CS.loc[ii,'CUSIP_CS']  =   ','.join(CS.loc[TempSearchIndex,'cusip'] \
                                                     .unique().tolist())
    if np.mod(ii+1,1000)==0:
        print(str(ii+1)+' search done, with target rate '+ \
              str(np.round(TempCounter/(ii+1)*100,2))+'%')

## Save the Link File
DataFolder      =   "..\\temp\\"
pickle.dump(Link_SDC_CS,open(DataFolder+"Link_SDC_Compustat.p","wb"))

# End of Section:
###############################################################################



#%% Import and Clean the Compustat Quarterly Data 

DataFolder      =   "..\\..\\..\\..\\..\\Data\\Compustat\\Quarterly\\"


### Firm Sample

## Import
FirmSample          =   pickle.load(open(DataFolder+"Compustat_Names.p","rb"))

## Clean Data
FirmSample          =   FirmSample[['gvkey','sic']]
FirmSample['sic']   =   pd.to_numeric(FirmSample['sic'],errors='coerce',downcast='integer')

## Restrict the Firm Sample by Sectors
# Eliminate the Firms in Financial Sector
TempInd_Finance     =   ( FirmSample['sic']>=6000 ) & (FirmSample['sic']<=6999)
# Utility Sector
TempInd_Utility     =   ( FirmSample['sic']>=4900 ) & (FirmSample['sic']<=4999)
# Quasi-government Sector
TempInd_QuasiGov    =   ( FirmSample['sic']>=9000 ) & (FirmSample['sic']<=9999)

TempInd_Drop        =   (TempInd_Finance | TempInd_Utility | TempInd_QuasiGov)
FirmSample          =   FirmSample[~TempInd_Drop] \
                        .dropna(subset=['gvkey']).reset_index(drop=True)

### Historical Record of Firms

## Import FUNDQ
FirmRecord          =   pickle.load(open(DataFolder+"Compustat_FundQ.p","rb"))

## Merge with the Selected Firm Sample
Sample              =   FirmSample[['gvkey']] \
                        .merge(right=FirmRecord,how='left',left_on='gvkey',right_on='gvkey') \
                        .drop_duplicates()
                        
## Check the Record
# Report Format: whether indfmt is 'INDL' 
Sample['IndFmt'].value_counts(normalize=True)
# Cash Flow Statement Format: 7 is the right Cash Flow Statement format
Sample['SCFQ'].value_counts(normalize=True)
TempCheck_SCFQ      =   Sample.groupby('CalendarQtr')['SCFQ'].value_counts(normalize=True)
'''
Starting from 1988Q4, firms start adopting the new reporting standard. Between
1988Q4 and 2000Q1, the percentage of SCFQ=7 is around 95%. Since 2000, almost 
all the firms use SCFQ=7.
'''
# Replicates of gvkey and CalendarQtr
Sample.groupby(['gvkey','CalendarQtr']).apply(lambda x: x.shape[0]).value_counts(normalize=True)
TempCheck_UniqueID  =   Sample.groupby(['gvkey','CalendarQtr']).apply(lambda x: x.shape[0])


### Clean the Sample

## Drop the Obs. without the key Information
Sample              =   Sample.dropna(subset=['gvkey','CalendarQtr','FiscalQtr']) \
                        .reset_index(drop=True)

## Date Variable Representation: Calendar and Fiscal Quarter
for QtrVar in ['CalendarQtr','FiscalQtr']:
    Sample[QtrVar+'_Quarter']   =   Sample[QtrVar].map(lambda x: int(x[-1]))
    Sample[QtrVar+'_Year']      =   Sample[QtrVar].map(lambda x: int(x[0:4]))
    Sample[QtrVar+'_Date']      =   Sample[QtrVar].map(lambda x: datetime.date(int(x[0:4]),int(x[-1])*3-2,1))
    Sample[QtrVar+'_Qnum']      =   Sample[QtrVar].map(lambda x: int(x[0:4])*4+int(x[-1]))

## Sort the Data
Sample              =   Sample.sort_values(by=['gvkey','FiscalQtr_Qnum']) \
                        .reset_index(drop=True)
for QtrVar in ['CalendarQtr','FiscalQtr']:
    Sample[QtrVar+'_Gap']   =   Sample.groupby('gvkey')[QtrVar+'_Qnum'].diff()

## Decompose the Accumulated Flow into the Quarterly Flow
AccFlowVarList      =   ['StockIssuance','StockRepurchase','Dividend', \
                         'LongTermDebt_Issuance','LongTermDebt_Reduction','CurrentDebt_NetIssuance']
QtrFlowVarList      =   [x+'_'+'QFlow' for x in AccFlowVarList]
Sample[QtrFlowVarList] \
                    =   Sample.groupby(['gvkey','FiscalQtr_Year'])[AccFlowVarList].diff()

Sample.loc[Sample['FiscalQtr_Gap']>1,QtrFlowVarList] \
                    =   np.nan
TempInd_FQ1         =   Sample[Sample['FiscalQtr_Quarter']==1].index

for ii in range(len(AccFlowVarList)):
    Sample.loc[TempInd_FQ1,QtrFlowVarList[ii]] \
                        =  Sample.loc[TempInd_FQ1,AccFlowVarList[ii]]

Sample              =   Sample.drop(AccFlowVarList,axis=1) \
                        .rename(columns={QtrFlowVarList[ii]: AccFlowVarList[ii] \
                                         for ii in range(len(AccFlowVarList))})

## Derived Variables
# Leverage
Sample['Leverage']  =   Sample['Liability']/Sample['Asset']
# Equity Market Value
Sample['Equity_MV'] =   Sample['CommonShares_Outstanding']*Sample['StockPrice_Close']
# Market to Book Value Ratio
Sample['Equity_M2B']=   Sample['Equity_MV']/Sample['Equity_Common']*100
# Current Asset as a Share of Total Asset
Sample['CurrentAssetRatio'] \
                    =   Sample['Asset_Current']/Sample['Asset']
# Size
Sample['Size']      =   Sample['Asset']
# Sales Growth
Sample['LogSales']  =   np.log(Sample['Sales'])
Sample['SaleGrowth']=   Sample.groupby('gvkey')['LogSales'].diff()
Sample['SaleGrowth']=   Sample['SaleGrowth']/Sample['FiscalQtr_Gap']
Sample['SaleGrowth']=   Sample['SaleGrowth'].replace([np.inf,-np.inf],np.nan)

## Size Growth
Sample['LogAsset']  =   np.log(Sample['Asset'])
Sample['AssetGrowth']=  Sample.groupby('gvkey')['LogAsset'].diff()
Sample['AssetGrowth']=  Sample['AssetGrowth']/Sample['FiscalQtr_Gap']
Sample['AssetGrowth']=  Sample['AssetGrowth'].replace([np.inf,-np.inf],np.nan)

                    
### Output to be Linked with SDC Record
VarList             =   ['gvkey','CalendarQtr','CalendarQtr_Date', \
                         'Asset','Equity_MV','Equity','CommonShares_Outstanding', \
                         'Leverage','CurrentAssetRatio', \
                         'Size', \
                         'SaleGrowth','AssetGrowth','Equity_M2B' \
                         ]

Output              =   Sample[VarList].replace([np.inf,-np.inf],np.nan) \
                        .rename(columns={'CalendarQtr_Date': 'Date'})

DataFolder          =   "..\\temp\\"
pickle.dump(Output,open(DataFolder+"CompustatQ.p","wb"))

# End of Section:
###############################################################################



#%% Refine the Compustat Sample based its Link with SDC Deals


### Load in Data Sets
DataFolder      =   "..\\temp\\"
Link_SDC_CS     =   pickle.load(open(DataFolder+"Link_SDC_Compustat.p","rb"))
CS              =   pickle.load(open(DataFolder+"CompustatQ.p","rb"))


### Clean the Link File
Link_SDC_CS     =   Link_SDC_CS[Link_SDC_CS['LinkNum']==1]
Link_SDC_CS     =   Link_SDC_CS[['IssueID','gvkey']].reset_index(drop=True)


### Clean the Compustat Quarterly Data

## Temporary Function to Winsorize Variables
def TempFun_Winsorize(InputS,LowQ,HighQ):
    S       =   InputS.copy()
    LowS    =   S.quantile(LowQ)
    HighS   =   S.quantile(HighQ)
    S[ (S<LowS) | (S>HighS) ]   =   np.nan
    return S

## Summary Statistics before Winsorization
SumStat_1       =   CS.describe(percentiles=[0.005,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.995])
SumStat_1.loc['count',:] \
                =   SumStat_1.loc['count',:]/CS.shape[0]

## Winsorize the Variables
# Variables with Clear Boundary
CS.loc[CS['Equity_MV']<=0,'Equity_MV'] \
                =   np.nan
CS.loc[CS['CommonShares_Outstanding']<=0,'CommonShares_Outstanding'] \
                =   np.nan
CS.loc[CS['CurrentAssetRatio']<0,'CurrentAssetRatio'] \
                =   np.nan
CS.loc[CS['CurrentAssetRatio']>1,'CurrentAssetRatio'] \
                =   np.nan
# Variables to be Winsorized by Extreme Quantiles
VarList         =   ['Leverage','SaleGrowth','AssetGrowth']

for Var in VarList:
    CS[Var]         =   TempFun_Winsorize(CS[Var],0.005,0.995)

## Summary Statistics after Winsorization
SumStat_2       =   CS.describe(percentiles=[0.005,0.01,0.1,0.25,0.5,0.75,0.9,0.99,0.995])
SumStat_2.loc['count',:] \
                =   SumStat_2.loc['count',:]/CS.shape[0]

## Generate the Quantile Category
QuantBin        =   [-0.01,0.25,0.75,1.01]  
# Variable List
TempVarList     =   ['Size','SaleGrowth','AssetGrowth','Equity_M2B']
# Generate the Quantile Categories
for TempVar in TempVarList:
    CS[TempVar+'Quant'] \
                    =   CS[~pd.isna(CS[TempVar])] \
                        .groupby('Date')[TempVar].rank(ascending=True,pct=True)
    CS[TempVar+'Type'] \
                =   pd.cut(CS[TempVar+'Quant'],bins=QuantBin,labels=range(1,len(QuantBin)))

# Generate the Quantile Category Dummy Variables
for Var in TempVarList:
    CS[Var+'_UppQuant']     =   0
    CS.loc[CS[Var+'Type']==3,Var+'_UppQuant'] \
                            =   1
    CS[Var+'_LowQuant']     =   0
    CS.loc[CS[Var+'Type']==1,Var+'_LowQuant'] \
                            =   1
### Merge the Sample
SDC_CS          =   Link_SDC_CS.merge(right=CS,how='left',on='gvkey')

pickle.dump(SDC_CS,open(DataFolder+"SDC_CompustatQ.p","wb"))
# End of Section：
###############################################################################
